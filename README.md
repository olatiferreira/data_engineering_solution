# Solu√ß√£o para Atualiza√ß√£o dos Dados de Forma Resiliente e Est√°vel üöÄ

## Objetivo üéØ

Tornar o processo de atualiza√ß√£o dos dados atrav√©s das camadas (bronze, silver e gold) mais resilientes e est√°vel, garantindo os SLAs necess√°rios para o neg√≥cio, prevenindo a introdu√ß√£o de m√°s pr√°ticas e erros em produ√ß√£o, sejam de implementa√ß√£o ou de configura√ß√£o. Em caso de detec√ß√£o de problemas, o desenvolvedor deve ser notificado e orientado sobre como proceder. 

## Tecnologias e Ferramentas üíª

- **Armazenamento:** [AWS S3](https://aws.amazon.com/pt/s3/)
- **Processamento:** [Databricks](https://www.databricks.com/br)
- **An√°lise:** [ThoughtSpot](https://www.thoughtspot.com/)
- **Orquestra√ß√£o:** [Airflow](https://airflow.apache.org/)
- **Controle de vers√£o:** [GitLab](https://about.gitlab.com/)

## Resumo üîé

1. Pipeline de dados automatizado que extrai dados de fontes originais e os armazena no S3;

2. O Databricks ser√° utilizado para processar e limpar os dados oriundos do S3, aplicando regras de valida√ß√£o, transforma√ß√µes e agregando dados de diferentes fontes;

3. Os dados refinados poder√£o ser integrados no ThoughtSpot para que os usu√°rios de neg√≥cios possam explor√°-los e analis√°-los;

4. O Airflow ir√° orquestrar o fluxo de trabalho de todo o pipeline, agendando tarefas de extra√ß√£o, processamento, carregamento e an√°lise, inclusive notificando e orientando o desenvolvedor em caso de falhas;

5. Testes de unidade e integra√ß√£o ser√£o implementados para garantir a qualidade e confiabilidade do pipeline;

6. O pipeline de dados e a arquitetura do produto anal√≠tico dever√° ser documentado de forma macro para facilitar a compreens√£o e a manuten√ß√£o pelo pr√≥prio desenvolvedor e/ou de seus pares.

## Arquitetura üé≤

![App Screenshot](https://via.placeholder.com/468x300?text=App+Screenshot+Here)


## Etapas üìÉ

#### 1. Coleta de Dados

- Os dados brutos ser√£o coletados de suas fontes originais e armazenados na camada bronze do S3, em formato parquet;

- Os metadados dos dados (fonte, data, schema, etc.) poder√£o ser armazenados em um banco de dados NoSQL, como por exemplo o DynamoDB, facilitando a consulta, gerenciamento e governan√ßa.

#### 2. Processamento

- O Databricks ser√° utilizado como plataforma de processamento de dados em nuvem, que al√©m de prover recursos de clusteriza√ß√£o, tamb√©m ir√° oferecer provisionamento autom√°tico para lidar com grandes volumes de dados para executar as tarefas de ETL (extra√ß√£o, transforma√ß√£o e carregamento), limpeza e prepara√ß√£o dos dados;

- Cada notebook ser√° executado em um cluster Databricks dedicado, aproveitando a escalabilidade e o poder de processamento da plataforma.

#### 3. Controle de Vers√£o e Colabora√ß√£o

- O GitLab ser√° utilizado como reposit√≥rio central para armazenar o c√≥digo do pipeline de dados, documenta√ß√£o e outros recursos relacionados (notebooks Python/SQL);

    - Ele facilitar√° a colabora√ß√£o entre equipes, a revis√£o de c√≥digo e o controle de vers√µes.

#### 4. Valida√ß√£o dos Dados

- Antes dos dados serem carregados para as pr√≥ximas camadas, os dados ser√£o validados para garantir a qualidade, consist√™ncia e integridade;

- Regras de valida√ß√£o customizadas poder√£o ser implementadas para cada tipo de dado e etapa do pipeline;

- Em caso de inconsist√™ncias, o desenvolvedor respons√°vel pelo notebook ser√° notificado via e-mail e/ou Slack;
    - A notifica√ß√£o poder√° incluir detalhes do erro, logs e instru√ß√µes sobre como corrigi-lo.

    #### 4.1. Teste Unit√°rio

    - Testes unit√°rios automatizados ser√£o implementados para garantir o funcionamento correto dos notebooks;

    - Os testes ser√£o executados antes de cada execu√ß√£o do pipeline no Databricks;

    - Falhas nos testes unit√°rios impedir√£o a execu√ß√£o do pipeline em produ√ß√£o, prevenindo a introdu√ß√£o de erros.

#### 5. Armazenamento dos Dados

- Para prover escalabilidade e durabilidade dos dados, eles ser√£o armazenados em camadas no AWS S3, conforme abaixo:

    - **Bronze:** Dados brutos na forma original.
    - **Silver:** Dados transformados.
    - **Gold:** Dados validados e prontos para an√°lise e uso pelas unidades de neg√≥cio, utilizando o ThoughtSpot por exemplo.

#### 6. An√°lise e Visualiza√ß√£o

- O ThoughtSpot ser√° utilizado como plataforma de an√°lise de dados self-service, fornecendo aos usu√°rios de neg√≥cio acesso interativo e visual aos dados da camada gold;

- Os usu√°rios poder√£o explorar os dados, criar dashboards e relat√≥rios personalizados, sem a necessidade de conhecimento t√©cnico aprofundado.

#### 7. Orquestra√ß√£o, Monitoramento e Governan√ßa

- O Airflow ir√° monitorar a execu√ß√£o dos notebooks e pipelines, notificando os desenvolvedores via e-mail e/ou Slack, em caso de falhas ou atrasos;

- Logs detalhados de cada etapa do processo ser√£o armazenados no AWS S3 para an√°lise e investiga√ß√£o em caso de problemas;

- Pol√≠ticas de acesso e seguran√ßa s√£o implementadas no AWS S3, Databricks e ThoughtSpot para controlar o acesso aos dados e garantir a confidencialidade e integridade dos dados.

    #### 7.1 Monitoramento de Desempenho

    - M√©tricas de desempenho dos pipelines (tempo de execu√ß√£o, volume de dados processados, taxa de erros, etc) ser√£o coletadas e armazenadas para fins de observabilidade;

    - Dashboards e alertas ser√£o configurados para monitorar as m√©tricas de desempenho e identificar gargalos ou anomalias;

     - A√ß√µes automatizadas poder√£o ser configuradas para escalar recursos ou reexecutar pipelines em caso de problemas de desempenho.

    #### 7.2 Governan√ßa de Dados

    - Um cat√°logo de dados ser√° implementado para documentar as fontes de dados, pipelines, tabelas e outros artefatos relacionados √† gest√£o de dados;

    - Pol√≠ticas de qualidade de dados ser√£o definidas e aplicadas para garantir a confiabilidade e a integridade dos dados;

    - Controles de acesso granular ser√£o implementados para restringir o acesso aos dados com base nas fun√ß√µes e responsabilidades dos usu√°rios.

## Trade-offs üîÑ

- **Custo:** A utiliza√ß√£o de ferramentas como Databricks e ThoughtSpot poder√£o gerar custos adicionais. No entanto, o retorno do investimento (ROI) da solu√ß√£o poder√° ser significativo em termos de maior efici√™ncia, produtividade e redu√ß√£o de riscos.

- **Complexidade:** A implementa√ß√£o da solu√ß√£o completa poder√° exigir um investimento inicial em treinamento e familiariza√ß√£o das equipes com as novas ferramentas. No entanto, a longo prazo, a solu√ß√£o ir√° contribuir para a padroniza√ß√£o dos processos, reduzindo o tempo de desenvolvimento e a necessidade de manuten√ß√£o dos pipelines.

## Benef√≠cios üéÅ

- **Armazenamento centralizado e seguro de dados:** O S3 garante que seus dados estejam sempre dispon√≠veis e protegidos.

- **Processamento de dados escal√°vel e eficiente:** O Databricks permite lidar com grandes volumes de dados de forma eficiente e escal√°vel.

- **An√°lise de dados self-service:** O ThoughtSpot capacita os usu√°rios de neg√≥cios a explorar e analisar dados sem a necessidade de conhecimento t√©cnico aprofundado.

- **Orquestra√ß√£o automatizada de tarefas:** O Airflow garante que o pipeline de dados seja executado de forma confi√°vel e consistente.

- **Controle de vers√£o e colabora√ß√£o:** O GitLab facilita a colabora√ß√£o entre equipes e o controle de vers√µes do c√≥digo do pipeline.

## Desenvolvido por ‚ú®

- [√çtalo C√©sar Ferreira da Costa](https://www.olatiferreira.com)