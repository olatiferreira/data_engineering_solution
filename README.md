# SoluÃ§Ã£o para AtualizaÃ§Ã£o dos Dados de Forma Resiliente e EstÃ¡vel ğŸš€

## Objetivo ğŸ¯

Tornar o processo de atualizaÃ§Ã£o dos dados atravÃ©s das camadas (bronze, silver e gold) mais resilientes e estÃ¡vel, garantindo os SLAs necessÃ¡rios para o negÃ³cio, prevenindo a introduÃ§Ã£o de mÃ¡s prÃ¡ticas e erros em produÃ§Ã£o, sejam de implementaÃ§Ã£o ou de configuraÃ§Ã£o. Em caso de detecÃ§Ã£o de problemas, o desenvolvedor deve ser notificado e orientado sobre como proceder. 

## Tecnologias e Ferramentas ğŸ’»

- **Armazenamento:** [AWS S3](https://aws.amazon.com/pt/s3/)
- **Processamento:** [Databricks](https://www.databricks.com/br)
- **AnÃ¡lise:** [ThoughtSpot](https://www.thoughtspot.com/)
- **OrquestraÃ§Ã£o:** [Airflow](https://airflow.apache.org/)
- **Versionamento e Pipeline CI/CD:** [GitLab](https://about.gitlab.com/)

## [EntregÃ¡vel 1] - SoluÃ§Ã£o detalhada

### Etapas ğŸ“ƒ

#### 1. Armazenamento

- Para prover escalabilidade e durabilidade dos dados, eles serÃ£o armazenados em camadas no AWS S3, conforme abaixo:

    - **Bronze:** Dados brutos na forma original.
    - **Silver:** Dados transformados.
    - **Gold:** Dados validados e prontos para anÃ¡lise e uso pelas unidades de negÃ³cio, utilizando o ThoughtSpot por exemplo.

- Os metadados dos dados (fonte, data, schema, etc.) poderÃ£o ser armazenados em um banco de dados NoSQL, como por exemplo o DynamoDB, facilitando a consulta, gerenciamento e governanÃ§a;

- Para reduzir custos e otimizar o armazenamento, serÃ£o implementadas polÃ­ticas de ciclo de vida dos dados, as quais irÃ£o permitir automatizar a transiÃ§Ã£o dos dados entre diferentes classes de armazenamento do S3 com base nos padrÃµes de acesso;

    -  Por exemplo, os dados que nÃ£o forem acessados apÃ³s um determinado perÃ­odo de tempo serÃ£o movidos para a classe de armazenamento S3 Infrequent Access (S3 IA) ou atÃ© mesmo para a S3 Glacier Flexible Retrieval.

#### 2. Processamento

- O Databricks serÃ¡ utilizado como plataforma de processamento de dados em nuvem, que alÃ©m de prover recursos de clusterizaÃ§Ã£o, tambÃ©m irÃ¡ oferecer provisionamento automÃ¡tico para lidar com grandes volumes de dados para executar as tarefas de ETL (extraÃ§Ã£o, transformaÃ§Ã£o e carregamento), limpeza e preparaÃ§Ã£o dos dados, o que significa que recursos poderÃ£o ser dimensionados de acordo com a demanda, reduzindo significativamente os custos operacionais;

    - Por exemplo, poderÃ¡ ser provisionado clusters temporÃ¡rios para execuÃ§Ã£o de tarefas apenas quando necessÃ¡rio, evitando gastos excessivos com infraestrutura ociosa;

- Cada notebook serÃ¡ executado em um cluster Databricks dedicado, conforme as boas prÃ¡ticas recomendam, aproveitando a escalabilidade e o poder de processamento da plataforma de forma eficiente, garantindo que apenas os recursos necessÃ¡rios sejam utilizados;

    - Por exemplo, poderÃ¡ ser configurado polÃ­ticas de auto escalonamento para ajustar dinamicamente o tamanho dos clusters com base na carga de trabalho, evitando desperdÃ­cio de recursos e consequentemente reduzindo os custos.

#### 3. AnÃ¡lise

- O ThoughtSpot serÃ¡ utilizado como plataforma de anÃ¡lise de dados self-service, fornecendo aos usuÃ¡rios de negÃ³cio acesso interativo e visual aos dados da camada gold;

- Os usuÃ¡rios poderÃ£o explorar os dados, criar dashboards e relatÃ³rios personalizados, sem a necessidade de conhecimento tÃ©cnico aprofundado.

#### 4. OrquestraÃ§Ã£o

- O Airflow serÃ¡ utilizado para agendar, monitorar e gerenciar os pipelines de dados;

- PolÃ­ticas de escalonamento dinÃ¢mico de recursos serÃ£o configuradas para aumentar ou diminuir o nÃºmero de workers do Airflow com base na carga de trabalho, otimizando os recursos de computaÃ§Ã£o, reduzindo significamente os custos;

- SerÃ¡ implementado scripts e ferramentas que desligam automaticamente instÃ¢ncias do Airflow quando nÃ£o estiverem em uso por um determinado perÃ­odo de tempo;

- Alertas serÃ£o configurados e poderÃ£o ser enviados no Slack, notificando o desenvolvedor sobre as mÃ©tricas de desempenho dos pipelines (tempo de execuÃ§Ã£o, volume de dados processados, taxa de erros, etc) e sobre o status de execuÃ§Ã£o das etapas dos pipelines, incluindo falhas;

- Todos os logs serÃ£o armazenados no S3 para fins anÃ¡lise e investigaÃ§Ã£o, possibilitando a identificaÃ§Ã£o de oportunidades de otimizaÃ§Ã£o e ajustes nas configuraÃ§Ãµes para reduzir custos sem comprometer a disponibilidade.

#### 5. Versionamento e Pipeline CI/CD

- O GitLab serÃ¡ utilizado como repositÃ³rio para gerenciamento, controle de versÃ£o e documentaÃ§Ã£o dos scripts, integraÃ§Ã£o contÃ­nua, entrega contÃ­nua e colaboraÃ§Ã£o entre equipes de desenvolvimento;

- SerÃ¡ configurado um pipeline que irÃ¡ executar testes automatizados, como testes unitÃ¡rios e testes de integraÃ§Ã£o, sempre que houver uma nova alteraÃ§Ã£o no cÃ³digo do repositÃ³rio;

- HaverÃ¡ a implementaÃ§Ã£o de um processo de revisÃ£o de cÃ³digo, onde os membros da equipe revisam e validam as alteraÃ§Ãµes propostas antes de serem integradas ao pipeline principal;

- Ferramentas de linting e verificadores de estilo de cÃ³digo serÃ£o utilizadas para garantir que o cÃ³digo do pipeline siga as melhores prÃ¡ticas e os padrÃµes estabelecidos;

- Ferramentas de anÃ¡lise estÃ¡tica de cÃ³digo serÃ£o configuradas para identificar potenciais problemas de qualidade, como cÃ³digo duplicado, complexidade excessiva e vulnerabilidades de seguranÃ§a.

#### 6. GovernanÃ§a de Dados

- Um catÃ¡logo de dados serÃ¡ implementado para documentar as fontes de dados, pipelines, tabelas e outros artefatos relacionados Ã  gestÃ£o de dados;

- PolÃ­ticas de acesso, qualidade e seguranÃ§a serÃ£o definidas e aplicadas para restringir e controlar o acesso aos dados, garantindo a confiabilidade, confidencialidade e integridade dos dados.

### Trade-offs ğŸ”„

- **Custo:** A utilizaÃ§Ã£o de ferramentas como Databricks e ThoughtSpot poderÃ£o gerar custos adicionais. No entanto, o retorno do investimento (ROI) da soluÃ§Ã£o poderÃ¡ ser significativo em termos de maior eficiÃªncia, produtividade e reduÃ§Ã£o de riscos.

- **Escalabilidade e Elasticidade:** A configuraÃ§Ã£o correta dos recursos de escalabilidade e elasticidade pode ser complexa. No entanto, uma vez configurados adequadamente, esses recursos permitem que os pipelines de dados se adaptem dinamicamente Ã  demanda, garantindo um desempenho consistente mesmo diante de variaÃ§Ãµes na carga de trabalho.

- **ServiÃ§os Gerenciados:** A soluÃ§Ã£o depende da disponibilidade e desempenho de serviÃ§os gerenciados como AWS S3, Databricks e ThoughtSpot. No entanto, permitirÃ¡ que a equipe se concentre nas tarefas de valor agregado, aproveitando a expertise e a infraestrutura desses provedores, o que pode resultar em uma implementaÃ§Ã£o mais rÃ¡pida e eficiente dos pipelines de dados.

- **Complexidade:** A implementaÃ§Ã£o da soluÃ§Ã£o completa poderÃ¡ exigir um investimento inicial em treinamento e familiarizaÃ§Ã£o das equipes com as novas ferramentas. No entanto, a longo prazo, a soluÃ§Ã£o irÃ¡ contribuir para a padronizaÃ§Ã£o dos processos, reduzindo o tempo de desenvolvimento e a necessidade de manutenÃ§Ã£o dos pipelines.

### BenefÃ­cios ğŸ

- **Armazenamento centralizado e seguro de dados:** O S3 garante que seus dados estejam sempre disponÃ­veis e protegidos.

- **EficiÃªncia operacional:** O Databricks permite lidar com grandes volumes de dados de forma eficiente e escalÃ¡vel, otimizando recursos e reduzindo custos operacionais.

- **AnÃ¡lise de dados self-service:** O ThoughtSpot capacita os usuÃ¡rios de negÃ³cios a explorar e analisar dados sem a necessidade de conhecimento tÃ©cnico aprofundado.

- **AutomaÃ§Ã£o e orquestraÃ§Ã£o:** O Airflow garante que o pipeline de dados seja executado de forma confiÃ¡vel e consistente, garantindo os SLAs necessÃ¡rios para o negÃ³cio.

- **Controle de versÃ£o e qualidade do cÃ³digo:** AlÃ©m do GitLab realizar o controle de versÃ£o e documentaÃ§Ã£o dos scripts, garantindo a colaboraÃ§Ã£o entre equipes de desenvolvimento, tambÃ©m implementa pipelines CI/CD automatizados, que juntamente com testes automatizados e de revisÃ£o de cÃ³digo, assegura a qualidade do cÃ³digo e a integridade dos pipelines.

## [EntregÃ¡vel 2] - ImplementaÃ§Ã£o prÃ¡tica de um componente demonstrando o funcionamento

O componente da soluÃ§Ã£o implementado para demonstrar o funcionamento foi a criaÃ§Ã£o e execuÃ§Ã£o de uma tarefa, utilizando o Amazon Managed Workflows for Apache **Airflow**, tornando o processo de atualizaÃ§Ã£o dos dados atravÃ©s das camadas mais resilientes e estÃ¡vel.

> **Nota**
> Esta implementaÃ§Ã£o trata-se de um MVP (produto viÃ¡vel mÃ­nimo) com o objetivo de demonstrar o funcionamento de um componente especÃ­fico da soluÃ§Ã£o citada anteriormente.

### Estrutura do Bucket no S3 ğŸ“

```sql
â”œâ”€â”€ data-engineering-solution-mvp
â”‚Â Â  â”œâ”€â”€ airflow
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people_pipeline.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scripts
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bronze
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people-100000.csv
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people-100000_v2.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ silver
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people_all.parquet
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gold
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people_without_duplicates.parquet
```

#### Dados

- Os dados da camada bronze sÃ£o fictÃ­cios, oriundos do repositÃ³rio do [Github](https://github.com/datablist/sample-csv-files?tab=readme-ov-file);

    - O download pode ser realizado [aqui](https://drive.google.com/uc?id=1VEi-dnEh4RbBKa97fyl_Eenkvu2NC6ki&export=download);

    - Os dados dos arquivos "people-100000.csv" e "people-100000_v2.csv" foram duplicados propositalmente para demonstrar a validaÃ§Ã£o realizada na etapa "write_gold_data" da tarefa "people_pipeline", mencionada abaixo.

### Tarefa (DAG) ğŸš¥

![DAG](./readme-img/1_dag.jpg)

### Fluxo ğŸ”€

![Fluxo](./readme-img/2_fluxo.jpg)

#### 1. Etapa "read_bronze_data"

![read_bronze_data](./readme-img/3_read_bronze_data.jpg)

- Esta etapa Ã© responsÃ¡vel por capturar os dados brutos na forma original de todos os arquivos armazenados na camada bronze;

    ![s3_read_bronze_data](./readme-img/3_1_s3_read_bronze_data.jpg)

    - A origem dos dados (bucket S3) Ã© configurada como uma variÃ¡vel no Airflow, portanto nÃ£o Ã© necessÃ¡rio alterar o script python do pipeline, facilitando a utilizaÃ§Ã£o por profissionais com baixo conhecimento tÃ©cnico e possibilitando a reutilizaÃ§Ã£o do cÃ³digo em outros pipelines de dados;

        ![var_read_bronze_data](./readme-img/3_3_var_read_bronze_data.jpg)

- A leitura dos arquivos Ã© realizada de forma dinÃ¢mica, independentemente da quantidade de arquivos existente e da nomenclatura deles;

- Ao final desta etapa uma notificaÃ§Ã£o Ã© enviada ao Slack sinalizando Ãªxito (quantidade de arquivos processados) ou falha, com os respectivos detalhes.

    ![slack_read_bronze_data](./readme-img/3_2_slack_read_bronze_data.jpg)

#### 2. Etapa "write_silver_data"

![write_silver_data](./readme-img/4_write_silver_data.jpg)

- Esta etapa Ã© responsÃ¡vel por capturar os dados processados na etapa anterior, consolidar e salvar em formato parquet na camada silver;

    ![s3_write_silver_data](./readme-img/4_1_s3_write_silver_data.jpg)

    - O nome do arquivo gerado Ã© configurado como uma variÃ¡vel no Airflow, portanto nÃ£o Ã© necessÃ¡rio alterar o script python do pipeline, facilitando a utilizaÃ§Ã£o por profissionais com baixo conhecimento tÃ©cnico e possibilitando a reutilizaÃ§Ã£o do cÃ³digo em outros pipelines de dados;

        ![var_write_silver_data](./readme-img/4_3_var_write_silver_data.jpg)

- Ao final desta etapa uma notificaÃ§Ã£o Ã© enviada ao Slack sinalizando Ãªxito (nome do arquivo gerado) ou falha, com os respectivos detalhes.

    ![slack_write_silver_data](./readme-img/4_2_slack_write_silver_data.jpg)

#### 3. Etapa "write_gold_data"

![write_gold_data](./readme-img/5_write_gold_data.jpg)

- Esta etapa Ã© responsÃ¡vel por capturar os dados processados na etapa anterior, retirar as duplicidades de registros e salva-los em formato parquet na camada gold para ser consumido pelas Ã¡reas de negÃ³cio;
    
    ![s3_write_gold_data](./readme-img/5_1_s3_write_gold_data.jpg)

    - O nome do arquivo gerado Ã© configurado como uma variÃ¡vel no Airflow, portanto nÃ£o Ã© necessÃ¡rio alterar o script python do pipeline, facilitando a utilizaÃ§Ã£o por profissionais com baixo conhecimento tÃ©cnico e possibilitando a reutilizaÃ§Ã£o do cÃ³digo em outros pipelines de dados;

        ![var_write_gold_data](./readme-img/5_3_var_write_gold_data.jpg)

- Ao final desta etapa uma notificaÃ§Ã£o Ã© enviada ao Slack sinalizando Ãªxito (nome do arquivo gerado e quantidade de registros duplicados identificado) ou falha, com os respectivos detalhes.
    
    ![slack_write_gold_data](./readme-img/5_2_slack_write_gold_data.jpg)

#### 4. ExecuÃ§Ã£o completa do fluxo

![fluxo_executado](./readme-img/6_fluxo_executado.jpg)

### Como Executar

#### 1. Crie um bucket no S3 conforme a estrutura abaixo

```sql
â”œâ”€â”€ <nome-do-seu-bucket>
â”‚Â Â  â”œâ”€â”€ airflow
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people_pipeline.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ scripts
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bronze
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people-100000.csv
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ people-100000_v2.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ silver
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gold
```

> **Nota**
> Os arquivos "people_pipeline.py" e "requirements.txt" estÃ£o disponibilizados neste repositÃ³rio e o download dos arquivos "people-100000.csv" e "people-100000_v2.csv" pode ser realizado [aqui](https://drive.google.com/uc?id=1VEi-dnEh4RbBKa97fyl_Eenkvu2NC6ki&export=download).

#### 2. Crie um ambiente no Amazon Managed Workflows for Apache Airflow

- Nas configuraÃ§Ãµes do ambiente da seÃ§Ã£o "CÃ³digo do DAG no Amazon S3":
    - Em "Bucket do S3", selecione o bucket criado no passo anterior;
    - Em "Pasta DAGs", selecione a pasta "dags" que estÃ¡ dentro do bucket criado no passo anterior;
    - Em "Arquivo de requisitos - opcional", selecione o arquivo "requirements.txt" que estÃ¡ dentro da pasta "scripts" criado dentro do bucket no passo anterior.
    
- Nas configuraÃ§Ãµes do ambiente da seÃ§Ã£o "Rede":
    - Em "Grupos de seguranÃ§a", crie um novo grupo de seguranÃ§a com as permissÃµes necessÃ¡rias para acessar a interface do usuÃ¡rio do Airflow;
        - Caso queira acessar a interface do usuÃ¡rio fora da rede corporativa, em "Acesso ao servidor web" selecione "Rede pÃºblica (Acesso Ã  Internet). Lembrando que estÃ¡ opÃ§Ã£o deixarÃ¡ o seu ambiente exposto publicamente.

> **Nota**
> ApÃ³s criar o ambiente do Airflow, serÃ¡ necessÃ¡rio acessar o IAM para conceder permissÃ£o na funÃ§Ã£o do Amazon Managed Workflows for Apache Airflow de leitura (getObjects) e escrita (putObjects) ao bucket S3 criado no passo 1.

#### 3. Acesse a interface do usuÃ¡rio do Airflow para configurar as variÃ¡veis e a conexÃ£o com o Slack para o envio das notificaÃ§Ãµes

- ConfiguraÃ§Ã£o de variÃ¡veis    

    - No menu superior da interface do usuÃ¡rio do Airflow, acesse "Admin -> Variables" e crie as variÃ¡veis abaixo:

        - `s3_bucket`: Nome do seu bucket S3.Sua chave de API para acessar o serviÃ§o.
        - `filename_silver`: Nome do arquivo que serÃ¡ gerado na camada silver.
        - `filename_gold`: Nome do arquivo que serÃ¡ gerado na camada gold.

            - Exemplo:

                ![variaveis](./readme-img/7_variaveis.jpg)

- ConfiguraÃ§Ã£o de conexÃ£o com o Slack    
    - No menu superior da interface do usuÃ¡rio do Airflow, acesse "Admin -> Connections" e crie a conexÃ£o abaixo:

        - `slack_webhook_connection`: Webhook URL criada referente ao canal do Slack.

            - Exemplo:

                ![slack](./readme-img/2_1_conn_slack.jpg)

    > **Nota**
    > SerÃ¡ necessÃ¡rio acessar as configuraÃ§Ãµes do Slack para ativar os webhooks de entrada e adicionar um novo webhook ao espaÃ§o de trabalho vinculado ao canal desejado. VocÃª pode conferir o passo a passo na [documentaÃ§Ã£o](https://api.slack.com/messaging/webhooks) do Slack.

#### 4. Execute a tarefa (DAG)

- Acesse a interface do usuÃ¡rio do Airflow;
- No menu superior da interface do usuÃ¡rio do Airflow, acesse "DAGs";
- A tarefa "people_pipeline" serÃ¡ importada e aparecerÃ¡ na interface do usuÃ¡rio do Airflow;
- Na tarefa, dentro da seÃ§Ã£o "Actions", clique no botÃ£o "Trigger DAG" para executÃ¡-la, conforme print abaixo.
    ![execute_tarefa](./readme-img/8_execute_tarefa.jpg)

#### 5. Valide a execuÃ§Ã£o da tarefa e se os dados foram gerados/atualizados nas respectivas camadas

- Verifique as notificaÃ§Ãµes que chegaram no canal do slack configurado no passo 3;
- Caso tenha recebido notificaÃ§Ãµes de Ãªxito, verifique se os dados foram gerados/atualizados nas camadas silver e gold dentro da pasta "data" criada no bucket no passo 1:
    - Verifique a existÃªncia do arquivo .parquet na camada silver;
    - Verifique a existÃªncia do arquivo .parquet na camada gold.

## Desenvolvido por âœ¨

- [Ãtalo CÃ©sar Ferreira da Costa](https://www.olatiferreira.com)